# Алгоритм

>[!tip] Программирование является привлекательным занятием не только потому, что оно перспективно с экономической и научной точек зрения, но и потому, что оно во многом может стать эстетическим опытом, как сочинение стихов или музыки.											 <br>Дональд Кнут<br>


Давайте начнём с простого вопроса: **что объединяет поход в магазин за хлебом, распознавание лица друга в толпе и развитие живого организма из крошечной клетки?** Во всех этих процессах есть своя логика, последовательность шагов, которую можно назвать **алгоритмом**.

Если обратиться к строгому определению, то **алгоритм – это точное предписание, которое определяет вычислительный процесс, ведущий от начальных данных к нужному результату.** Но если говорить проще, алгоритм — это **чёткий и конечный набор правил** для решения конкретной проблемы.

На самом деле, мы постоянно используем алгоритмы в повседневной жизни, часто даже не осознавая этого. Давайте рассмотрим простой пример: **«Как купить хлеб?»**.

Кажется, всё просто: взять деньги, выйти из дома, дойти до магазина, купить хлеб. Но давайте усложним:
*   Если на улице дождь — взять зонт.
*   Если подъезжает автобус — проехать одну остановку.
*   А если в магазине перерыв? Это уже **исключительная ситуация**.

Как видите, даже в такой бытовой задаче возникает множество условий. Человек легко с ними справляется благодаря интуиции и опыту. Но для компьютера каждый такой шаг и каждое «если» должны быть описаны максимально чётко. Как метко заметил Альберт Эйнштейн: **«Всё должно быть изложено так просто, как только возможно, но не проще»**. Это идеальный принцип, которым мы будем руководствоваться при создании алгоритмов.

Человечество для решения задач разработало разные подходы:
1.  **Алгоритмический (дискретный)** — основа современных компьютеров.
2.  **Непрерывный (интуитивный)** — свойственный человеку, а также аналоговым системам и нейронным сетям.
3.  **Квантово-механический** — для квантовых компьютеров.
4.  **Метод самоорганизации** — как в природе, когда из одной клетки по заложенной в ДНК «программе» развивается сложный организм.

**Так зачем же нам, особенно в программировании, нужно глубоко понимать алгоритмы?**

Дело в том, что человечество уже накопило огромную библиотеку решений для самых разных задач. Прежде чем изобретать велосипед, всегда стоит поискать готовые, проверенные и, что очень важно, **эффективные** алгоритмы, которые уже умеют:
*   Молниеносно сортировать огромные массивы данных.
*   Быстро находить нужную информацию.
*   Работать с сложными структурами, например, графами.
*   Оптимизировать бизнес-процессы и логистику.


# Поиск

Поиск определенной информации в памяти является ключевой операцией в вычислениях. Программисту очень важно владеть алгоритмами поиска. Самый простой из них — последовательный поиск: вы просматриваете все элементы один за другим, пока не будет найден нужный; как вариант, вы можете проверить все элементы, чтобы понять, что искомого среди них нет. Легко заметить, что последовательный поиск имеет сложность $O(n)$, где $n$ — это общее количество элементов в пространстве поиска. Но на случай, когда элементы хорошо структурированы, есть более эффективные алгоритмы. В разделе «Структуры»  мы убедились, что извлечение данных, представленных в формате сбалансированного двоичного дерева поиска, стоит всего $O(\log n)$. Если ваши элементы хранятся в сортированном массиве, то их можно отыскать за такое же время, $O(\log n)$, посредством двоичного поиска. Этот алгоритм на каждом шаге отбрасывает половину пространства поиска:

```java
function binary_search(items, key)
	if not items
		return NULL
	i <- items.length / 2
	if key = items[i] 
		return items[i] 
	if key > items[i] 
		sliced <- items.slice(i+1, items.length) 
	else 
		sliced <- items.slice(0, i-1) 
	return binary_search(sliced, key)
```

На каждом шаге алгоритм `binary_search` выполняет постоянное число операций и отбрасывает половину входных данных. Это означает, что для n элементов пространство поиска сведется к нулю за $\log_2n$ шагов. Поскольку на каждом шаге выполняется постоянное количество операций, алгоритм имеет сложность $O(\log n)$. Вы можете выполнять поиск среди миллиона или триллиона элементов, и этот алгоритм по-прежнему будет показывать хорошую производительность.


# О бОльшое

**O-большое** — это специальная нотация, которая описывает, как **скорость работы алгоритма** изменяется в зависимости от размера входных данных. 

Зачем это нужно? На практике вам постоянно придется выбирать между разными алгоритмами для решения одной и той же задачи. O-большое помогает объективно сравнить их эффективность и предсказать, как они будут работать с большими объемами данных.

Главная идея в том, что время выполнения алгоритмов растет с **разной скоростью** при увеличении размера входных данных (`n`).

**Классический пример: Простой поиск vs. Бинарный поиск**

*   **Простой поиск (линейный):** В худшем случае нужно проверить каждый из `n` элементов. Если проверка одного элемента занимает 1 мс, то для 100 элементов потребуется 100 мс, а для 1 000 000 000 элементов — 1 000 000 000 мс (это примерно **11 дней**).
*   **Бинарный поиск (логарифмический):** В худшем случае нужно проверить $\log_2(n)$ элементов. Для тех же 1 000 000 000 элементов это всего около 30 проверок (т.к. $2^{30}$ ≈ 1 млрд), что займет примерно **30 мс**.

Вывод: При росте `n` бинарный поиск начинает работать **не просто быстрее, а экспоненциально быстрее**. Для миллиарда элементов он оказывается в миллионы раз эффективнее.


O-большое **не измеряет время в секундах**. Оно подсчитывает **количество операций**, которые алгоритму придется выполнить в худшем случае, и показывает, как это количество растет с увеличением `n`.

*   **Простой поиск** требует `n` операций. Его сложность: **O(n)**.
*   **Бинарный поиск** требует `log n` операций. Его сложность: **O(log n)**.

**Задача:** Нарисовать сетку из 16 квадратов.

*   **Алгоритм 1: Рисовать каждый квадрат по отдельности.**
    *   Количество операций (рисование одного квадрата): 16.
    *   Сложность: **O(n)**. Для `n` квадратов нужно `n` шагов.

*   **Алгоритм 2: Складывать лист бумаги.**
    *   Операция: одно сложение. Каждое сложение удваивает количество прямоугольников.
    *   Чтобы получить 16 квадратов ($2^4 = 16$), нужно всего 4 сложения.
    *   Сложность: **O(log n)**. Для `n` квадратов нужно $\log_2(n)$ шагов.

Этот пример показывает, насколько эффективнее могут быть алгоритмы с O(log n) по сравнению с O(n).


O-большое всегда оценивает производительность в **худшем случае**. Это даёт нам гарантию и понимание "потолка" производительности.

*   **Пример:** Если вы ищете имя в телефонной книге с помощью простого поиска (O(n)), и оно оказывается первым, вы нашли его за 1 операцию. Но в худшем случае (если имя последнее) вам придется просмотреть все `n` имен. Поэтому сложность алгоритма — O(n), а не O(1).


Вот наиболее часто встречающиеся классы сложности:

| Сложность      | Название                      | Как растет количество операций при увеличении `n`               |
| :------------- | :---------------------------- | :-------------------------------------------------------------- |
| **O(1)**       | Константное время             | Не зависит от размера данных                                    |
| **O(log n)**   | Логарифмическое время         | Растет очень медленно                                           |
| **O(n)**       | Линейное время                | Растет пропорционально `n`                                      |
| **O(n log n)** | Линейно-логарифмическое время | Растет чуть быстрее, чем линейно                                |
| **O(n^2)**     | Квадратичное время            | Растет значительно быстрее (для `n=100` нужно ~10 000 операций) |
| **O(n!)**      | Факториальное время           | Растет катастрофически быстро, непригодно для больших `n`       |


![image](images/big_O.png)



1.  **Сравниваем операции, а не секунды:** O-большое сравнивает алгоритмы по количеству операций, а не по абсолютному времени выполнения, которое зависит от мощности компьютера.
2.  **Смотрим на рост:** Нотация показывает, насколько быстро ухудшается производительность алгоритма с ростом размера входных данных.
3.  **Ориентируемся на худший случай:** Это дает нам надежную верхнюю границу времени выполнения.
4.  **O(log n) — друг программиста:** Алгоритмы с такой сложностью невероятно эффективны на больших данных и являются целью для многих оптимизаций.

### O(1)

```python
def is_even(number):
    """Проверка числа на четность"""
    return number % 2 == 0

def get_list_info(arr):
    """Получение основной информации о списке"""
    return {
        'length': len(arr),
        'is_empty': len(arr) == 0,
        'has_elements': len(arr) > 0
    }

# Пример использования
numbers = [1, 2, 3, 4, 5]
print(f"Число 5 четное: {is_even(5)}")  # False
print(f"Информация о списке: {get_list_info(numbers)}")
```

### O(log n)

```python
def find_power_of_two(n):
    """Нахождение наибольшей степени двойки, не превышающей n"""
    power = 0
    current = 1
    while current * 2 <= n:
        current *= 2
        power += 1
    return power, current

def integer_square_root(n):
    """Нахождение целого квадратного корня методом бисекции"""
    if n < 2:
        return n
    
    left, right = 1, n
    while left <= right:
        mid = (left + right) // 2
        if mid * mid == n:
            return mid
        elif mid * mid < n:
            left = mid + 1
        else:
            right = mid - 1
    return right

# Пример использования
number = 100
power, value = find_power_of_two(number)
sqrt_val = integer_square_root(number)
print(f"Степень двойки для {number}: 2^{power} = {value}")
print(f"Целый квадратный корень из {number}: {sqrt_val}")
```

#### O(n)

```python
def find_max(arr):
    """Поиск максимального элемента в массиве"""
    if not arr:
        return None
    
    max_val = arr[0]
    for num in arr:
        if num > max_val:
            max_val = num
    return max_val

def count_occurrences(arr, target):
    """Подсчет количества вхождений элемента в массив"""
    count = 0
    for element in arr:
        if element == target:
            count += 1
    return count

# Пример использования
numbers = [3, 7, 2, 7, 1, 7, 4]
max_number = find_max(numbers)
seven_count = count_occurrences(numbers, 7)
print(f"Максимальный элемент: {max_number}")
print(f"Количество семерок: {seven_count}")
```

### O(n log n)

```python
def custom_sort_with_comparison(arr):
    """Сортировка с использованием сложных сравнений"""
    # Эмуляция сложной операции сравнения
    def complex_compare(x, y):
        # Сравниваем по последней цифре, затем по величине
        x_last = x % 10
        y_last = y % 10
        if x_last != y_last:
            return x_last - y_last
        return x - y
    
    # Используем встроенную сортировку с кастомным компаратором
    return sorted(arr, key=lambda x: (x % 10, x))

def find_unique_pairs(arr):
    """Нахождение всех уникальных пар с определенным свойством"""
    # Сначала сортируем для удобства работы с парами
    sorted_arr = sorted(arr)
    pairs = []
    
    # Находим пары, где сумма цифр первого равна сумме цифр второго
    def digit_sum(n):
        return sum(int(d) for d in str(abs(n)))
    
    for i in range(len(sorted_arr)):
        for j in range(i + 1, len(sorted_arr)):
            if digit_sum(sorted_arr[i]) == digit_sum(sorted_arr[j]):
                pairs.append((sorted_arr[i], sorted_arr[j]))
    
    return pairs

# Пример использования
numbers = [123, 45, 67, 89, 234, 56, 78]
sorted_nums = custom_sort_with_comparison(numbers)
unique_pairs = find_unique_pairs(numbers)
print(f"Особенная сортировка: {sorted_nums}")
print(f"Пары с равными суммами цифр: {unique_pairs}")
```

### O(n^2)

```python
def find_all_pairs_sum(arr, target_sum):
    """Поиск всех пар чисел, дающих в сумме заданное значение"""
    pairs = []
    for i in range(len(arr)):
        for j in range(i + 1, len(arr)):
            if arr[i] + arr[j] == target_sum:
                pairs.append((arr[i], arr[j]))
    return pairs

def matrix_transpose(matrix):
    """Транспонирование матрицы (без использования встроенных функций)"""
    if not matrix:
        return []
    
    rows = len(matrix)
    cols = len(matrix[0])
    
    # Создаем новую матрицу для результата
    result = [[0 for _ in range(rows)] for _ in range(cols)]
    
    for i in range(rows):
        for j in range(cols):
            result[j][i] = matrix[i][j]
    
    return result

# Пример использования
numbers = [1, 2, 3, 4, 5]
target = 6
pairs = find_all_pairs_sum(numbers, target)

matrix = [
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
]
transposed = matrix_transpose(matrix)

print(f"Пары с суммой {target}: {pairs}")
print("Транспонированная матрица:")
for row in transposed:
    print(row)
```

### O(n!)

```python
def generate_sequences(n, k):
    """Генерация всех последовательностей длины k из n элементов (с повторениями)"""
    if k == 0:
        return [[]]
    
    sequences = []
    smaller_sequences = generate_sequences(n, k - 1)
    
    for seq in smaller_sequences:
        for i in range(n):
            sequences.append(seq + [i])
    
    return sequences

def count_derangements(n):
    """Подсчет количества беспорядков (derangements) - перестановок без неподвижных точек"""
    if n == 0:
        return 1
    if n == 1:
        return 0
    
    # Рекурсивная формула: !n = (n-1) * (!(n-1) + !(n-2))
    return (n - 1) * (count_derangements(n - 1) + count_derangements(n - 2))

def recursive_permutation_count(n):
    """Рекурсивный подсчет количества перестановок"""
    if n <= 1:
        return 1
    # Каждый вызов порождает n рекурсивных вызовов
    total = 0
    for i in range(n):
        total += recursive_permutation_count(n - 1)
    return total

# Пример использования (используем небольшие n!)
print("Количество последовательностей длины 2 из 3 элементов:")
sequences = generate_sequences(3, 2)
print(f"Всего: {len(sequences)}")
for seq in sequences[:10]:  # Покажем только первые 10
    print(seq)

print(f"\nКоличество беспорядков для n=5: {count_derangements(5)}")
print(f"Рекурсивный подсчет перестановок для n=4: {recursive_permutation_count(4)}")
```

## Пример определения O-большого

Хорошим примером для демонстрации алгоритмов с разным порядком роста (разной сложностью) является классическая строковая задача — определить, является ли одно слово анаграммой другого. Одна строка будет анаграммой другой, если вторая строка может быть получена простой перестановкой букв первой. Например, `'heart'` и `'earth'` — это анаграммы. Как и строки `'python'` и `'typhon'`. Для простоты будем полагать, что обе заданные строки имеют одинаковую длину и составлены из набора из 26 букв английского алфавита в нижнем регистре. Наша цель — написать булеву функцию, которая принимает две строки и возвращает ответ на вопрос, являются ли они анаграммами.

### Решение 1: Проверка вхождения (Метки)

Первое решение задачи о анаграмме будет проверять, входит ли каждый символ первой строки во вторую. Если все символы будут «отмечены» как найденные, то строки являются анаграммами. «Пометка» символа будет выполняться путем замены его на специальное значение Python `None`. Однако, поскольку строки в Python неизменяемы (иммутабельны), первым шагом обработки станет преобразование второй строки в список. Каждый символ из первой строки может быть сверен с элементами списка, и, если будет найден, отмечен указанной заменой.

```python
FUNCTION anagramSolution1(s1, s2)
    // Базовая проверка: если длины разные — не анаграммы
    IF length(s1) != length(s2) THEN
        RETURN false
    END IF

    // Преобразуем вторую строку в изменяемый список
    alist <- list of characters from s2
    stillOK <- true
    pos1 <- 0

    // Проходим по каждому символу первой строки
    WHILE pos1 < length(s1) AND stillOK DO
        found <- false
        pos2 <- 0

        // Ищем текущий символ s1[pos1] в списке alist
        WHILE pos2 < length(alist) AND NOT found DO
            IF s1[pos1] == alist[pos2] THEN
                found <- true
            ELSE
                pos2 <- pos2 + 1
            END IF
        END WHILE

        // Если символ найден — помечаем его (например, как null)
        IF found THEN
            alist[pos2] <- null
        ELSE
            stillOK <- false
        END IF

        pos1 <- pos1 + 1
    END WHILE

    RETURN stillOK
END FUNCTION
```

При анализе этого алгоритма стоит отметить, что каждый из `n` символов в `s1` вызовет внутренний цикл по `n` символам списка, полученного из `s2`. В худшем случае количество сравнений можно выразить как сумму целых чисел от 1 до `n`. Как известно, эта сумма записывается так:

$$
\sum_{i=1}^{n} i = \frac{n(n + 1)}{2} = \frac{1}{2}n^2 + \frac{1}{2}n
$$

При увеличении `n` слагаемое $n^2$ становится доминирующим, поэтому слагаемым $n$ и константой $\frac{1}{2}$ можно пренебречь. Таким образом, асимптотическая сложность данного решения — **$O(n^2)$**.

### Решение 2: Сортировка и сравнение

Следующее решение использует тот факт, что даже если `s1` и `s2` различны как последовательности символов, они будут анаграммами только в том случае, если состоят из одинаковых символов. Следовательно, если мы отсортируем символы каждой строки в алфавитном порядке, то в результате должны получить идентичные строки (при условии, что `s1` и `s2` — анаграммы).

```python
FUNCTION anagramSolution2(s1, s2)
    // Если длины не совпадают — сразу не анаграммы
    IF length(s1) != length(s2) THEN
        RETURN false
    END IF

    // Преобразуем строки в списки символов
    alist1 <- list of characters from s1
    alist2 <- list of characters from s2

    // Сортируем оба списка (в алфавитном порядке)
    sort(alist1)
    sort(alist2)

    pos <- 0
    matches <- true

    // Поэлементно сравниваем отсортированные списки
    WHILE pos < length(s1) AND matches DO
        IF alist1[pos] == alist2[pos] THEN
            pos <- pos + 1
        ELSE
            matches <- false
        END IF
    END WHILE

    RETURN matches
END FUNCTION
```

На первый взгляд может показаться, что сложность этого алгоритма равна $O(n)$, поскольку он содержит один простой цикл для сравнения `n` символов после сортировки. Однако, два вызова метода `sort` вносят значительный вклад в общую сложность. Как правило, алгоритмы сортировки имеют сложность $O(n^2)$ или $O(n \log n)$, и именно эта операция становится доминирующей. Таким образом, итоговая сложность алгоритма будет определяться сложностью операции сортировки и также составит $O(n \log n)$ или $O(n^2)$.

### Решение 3: Метод полного перебора

Техника **полного перебора** (brute force) обычно используется, когда все другие разумные возможности исчерпаны. Для задачи о анаграмме мы могли бы сгенерировать список всех возможных строк, которые можно составить из символов `s1`, и проверить, входит ли `s2` в этот список. Однако у этого подхода есть серьезный недостаток: при генерации всех перестановок символов `s1` для первой позиции есть `n` вариантов, для второй — `n-1`, и так далее. Общее количество строк-кандидатов будет равно $n \times (n-1) \times (n-2) \times ... \times 3 \times 2 \times 1$, то есть **$n!$** (факториал от `n`).

Решение с сложностью $O(n!)$ при увеличении `n` растет даже быстрее, чем $O(2^n)$. Например, если длина `s1` составляет 20 символов, то количество возможных перестановок будет $20! = 2,432,902,008,176,640,000$. Если предположить, что мы можем обрабатывать одну комбинацию в секунду, то на перебор всех вариантов уйдет около 77 миллиардов лет. Очевидно, что это непрактичное решение.

```python
FUNCTION anagramSolutionBruteForce(s1, s2)
    // Быстрая проверка длины
    IF length(s1) != length(s2) THEN
        RETURN false
    END IF

    // Вспомогательная рекурсивная функция для генерации перестановок
    PROCEDURE generatePermutations(chars, current, used, allPerms)
        IF length(current) == length(chars) THEN
            // Добавляем собранную перестановку как строку
            allPerms.append(string formed from current)
            RETURN
        END IF

        FOR i FROM 0 TO length(chars) - 1 DO
            IF NOT used[i] THEN
                used[i] <- true
                current.append(chars[i])
                generatePermutations(chars, current, used, allPerms)
                current.removeLast()   // откат (backtrack)
                used[i] <- false
            END IF
        END FOR
    END PROCEDURE

    chars <- list of characters from s1
    allPerms <- empty list
    used <- array of false values with length = length(s1)

    generatePermutations(chars, empty list, used, allPerms)

    // Проверяем, есть ли s2 среди всех перестановок
    FOR each perm IN allPerms DO
        IF perm == s2 THEN
            RETURN true
        END IF
    END FOR

    RETURN false
END FUNCTION
```


### Решение 4: Подсчет и сравнение символов

Последнее решение использует преимущество того, что любые две анаграммы имеют одинаковое количество вхождений каждой буквы. Чтобы проверить это, мы можем подсчитать, сколько раз встречается каждый символ в обеих строках. Поскольку алфавит состоит из 26 букв, мы можем использовать два списка из 26 счетчиков — по одному для каждой строки. Каждый раз, встречая определенную букву, мы увеличиваем соответствующий ей счетчик.

```python
FUNCTION anagramSolution4(s1, s2)
    // Предполагается, что строки содержат только строчные буквы 'a'–'z'
    IF length(s1) != length(s2) THEN
        RETURN false
    END IF

    // Создаём два счётчика по 26 элементов (по одной ячейке на букву)
    c1 <- array of 26 zeros
    c2 <- array of 26 zeros

    // Подсчитываем частоты символов в s1
    FOR i FROM 0 TO length(s1) - 1 DO
        pos <- ASCII code of s1[i] - ASCII code of 'a'
        c1[pos] <- c1[pos] + 1
    END FOR

    // Подсчитываем частоты символов в s2
    FOR i FROM 0 TO length(s2) - 1 DO
        pos <- ASCII code of s2[i] - ASCII code of 'a'
        c2[pos] <- c2[pos] + 1
    END FOR

    // Сравниваем два массива счётчиков
    j <- 0
    stillOK <- true
    WHILE j < 26 AND stillOK DO
        IF c1[j] == c2[j] THEN
            j <- j + 1
        ELSE
            stillOK <- false
        END IF
    END WHILE

    RETURN stillOK
END FUNCTION
```

Это решение снова содержит несколько циклов, но, в отличие от первого варианта, ни один из них не является вложенным. Первые два цикла, используемые для подсчета символов, выполняются за время, пропорциональное `n` (то есть $O(n)$). Третий цикл, сравнивающий два списка счетчиков, всегда выполняется за фиксированное число итераций (26, так как в алфавите 26 букв), то есть его сложность $O(1)$. Суммируя, получаем $T(n) = 2n + 26$ шагов, что является **$O(n)$**. Мы нашли алгоритм с линейной сложностью для решения нашей задачи.

Прежде чем завершить пример, стоит сказать несколько слов о **пространственной сложности**. Хотя последнее решение работает за линейное время, оно достигает этого за счет использования дополнительной памяти для хранения двух списков счетчиков. Другими словами, этот алгоритм **жертвует пространством ради выигрыша во времени**.

Это очень распространенный в информатике компромисс («time-space tradeoff»). В данном случае затраты памяти невелики. Однако если бы алфавит состоял из миллионов символов, это стало бы проблемой. При выборе алгоритма именно вам, как специалисту, предстоит определить наилучшее использование доступных вычислительных ресурсов для конкретной задачи.


# Сортировка 

До появления компьютеров сортировка данных была известной проблемой, ее ручное выполнение занимало колоссальное количество времени. Когда в 1890-х годах компания Tabulating Machine Company (которая позже стала называться IBM) автоматизировала операции сортировки, это позволило на несколько лет быстрее обработать данные переписи населения США. Существует много алгоритмов сортировки. Более простые имеют временную сложность $O(n^2)$ **Сортировка выбором** — один из таких алгоритмов. Именно его люди предпочитают использовать для сортировки физической колоды карт. Сортировка выбором принадлежит многочисленной группе алгоритмов с квадратичной стоимостью. Мы, как правило, используем их для упорядочивания наборов данных, состоящих меньше чем из 1000 элементов. Одним из известных алгоритмов является сортировка вставками. Он показывает очень хорошую эффективность в сортировке уже почти упорядоченных наборов данных даже очень большого объема.

```java
function insertion_sort(list) 
	for i <- 2 … list.length 
		j <- i 
		while j and list[j-1] > list[j] 
			list.swap_items(j, j-1) 
			j <- j - 1
```

Выполните этот алгоритм на бумаге, с использованием большей частью отсортированного списка чисел. Для массивов, где не упорядочено незначительное число элементов, insertion_sort имеет сложность $O(n)$. В этом случае он выполняет меньше операций, чем какой-либо другой алгоритм сортировки. В отношении крупных наборов данных, о которых нельзя сказать, что они отсортированы большей частью, алгоритмы с временной сложностью $O(n^2)$ оказываются слишком медленными. Здесь нам нужны более эффективные подходы. Самыми известными высокоскоростными алгоритмами сортировки являются сортировка слиянием и так называемая быстрая сортировка, оба имеют сложность $O(n\log n)$. Вот как алгоритм быстрой сортировки раскладывает по порядку колоду карт.

1. Если в колоде менее четырех карт, то упорядочить их — и работа завершена. В противном случае перейти к шагу 2.
2. Вынуть из колоды наугад любую карту, которая становится опорной.
3. Карты со значением больше, чем у опорной, кладутся в новую колоду справа; карты с меньшим значением кладутся в новую колоду слева.
4. Проделать эту процедуру для каждой из двух только что созданных колод.
5. Объединить левую колоду, опорную карту и правую колоду, чтобы получить отсортированную колоду.

![images](images/card_deck.png)

Перетасуйте колоду карт и проделайте описанные шаги. Это поможет вам опробовать на практике алгоритм быстрой сортировки, а заодно укрепит ваше понимание рекурсии. Теперь вы готовы решать большинство задач, связанных с сортировкой. Здесь мы осветили не все алгоритмы сортировки, так что просто помните, что их гораздо больше и каждый из них соответствует конкретным задачам


# Поиск

Поиск определенной информации в памяти является ключевой операцией в вычислениях. Программисту очень важно владеть алгоритмами поиска. Самый простой из них — последовательный поиск: вы просматриваете все элементы один за другим, пока не будет найден нужный; как вариант, вы можете проверить все элементы, чтобы понять, что искомого среди них нет. Легко заметить, что последовательный поиск имеет сложность $O(n)$, где $n$ — это общее количество элементов в пространстве поиска. Но на случай, когда элементы хорошо структурированы, есть более эффективные алгоритмы. В разделе «Структуры»  мы убедились, что извлечение данных, представленных в формате сбалансированного двоичного дерева поиска, стоит всего $O(\log n)$. Если ваши элементы хранятся в сортированном массиве, то их можно отыскать за такое же время, $O(\log n)$, посредством двоичного поиска. Этот алгоритм на каждом шаге отбрасывает половину пространства поиска:

```java
function binary_search(items, key)
	if not items
		return NULL
	i <- items.length / 2
	if key = items[i] 
		return items[i] 
	if key > items[i] 
		sliced <- items.slice(i+1, items.length) 
	else 
		sliced <- items.slice(0, i-1) 
	return binary_search(sliced, key)
```

На каждом шаге алгоритм `binary_search` выполняет постоянное число операций и отбрасывает половину входных данных. Это означает, что для n элементов пространство поиска сведется к нулю за $\log_2n$ шагов. Поскольку на каждом шаге выполняется постоянное количество операций, алгоритм имеет сложность $O(\logn)$. Вы можете выполнять поиск среди миллиона или триллиона элементов, и этот алгоритм по-прежнему будет показывать хорошую производительность.
