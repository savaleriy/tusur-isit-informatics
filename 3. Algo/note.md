# Алгоритм

>[!tip] Программирование является привлекательным занятием не только потому, что оно перспективно с экономической и научной точек зрения, но и потому, что оно во многом может стать эстетическим опытом, как сочинение стихов или музыки.											 <br>Дональд Кнут<br>


Давайте начнём с простого вопроса: **что объединяет поход в магазин за хлебом, распознавание лица друга в толпе и развитие живого организма из крошечной клетки?** Во всех этих процессах есть своя логика, последовательность шагов, которую можно назвать **алгоритмом**.

Если обратиться к строгому определению, то **алгоритм – это точное предписание, которое определяет вычислительный процесс, ведущий от начальных данных к нужному результату.** Но если говорить проще, алгоритм — это **чёткий и конечный набор правил** для решения конкретной проблемы.

На самом деле, мы постоянно используем алгоритмы в повседневной жизни, часто даже не осознавая этого. Давайте рассмотрим простой пример: **«Как купить хлеб?»**.

Кажется, всё просто: взять деньги, выйти из дома, дойти до магазина, купить хлеб. Но давайте усложним:
*   Если на улице дождь — взять зонт.
*   Если подъезжает автобус — проехать одну остановку.
*   А если в магазине перерыв? Это уже **исключительная ситуация**.

Как видите, даже в такой бытовой задаче возникает множество условий. Человек легко с ними справляется благодаря интуиции и опыту. Но для компьютера каждый такой шаг и каждое «если» должны быть описаны максимально чётко. Как метко заметил Альберт Эйнштейн: **«Всё должно быть изложено так просто, как только возможно, но не проще»**. Это идеальный принцип, которым мы будем руководствоваться при создании алгоритмов.

Человечество для решения задач разработало разные подходы:
1.  **Алгоритмический (дискретный)** — основа современных компьютеров.
2.  **Непрерывный (интуитивный)** — свойственный человеку, а также аналоговым системам и нейронным сетям.
3.  **Квантово-механический** — для квантовых компьютеров.
4.  **Метод самоорганизации** — как в природе, когда из одной клетки по заложенной в ДНК «программе» развивается сложный организм.

**Так зачем же нам, особенно в программировании, нужно глубоко понимать алгоритмы?**

Дело в том, что человечество уже накопило огромную библиотеку решений для самых разных задач. Прежде чем изобретать велосипед, всегда стоит поискать готовые, проверенные и, что очень важно, **эффективные** алгоритмы, которые уже умеют:
*   Молниеносно сортировать огромные массивы данных.
*   Быстро находить нужную информацию.
*   Работать с сложными структурами, например, графами.
*   Оптимизировать бизнес-процессы и логистику.

Понимание алгоритмов — это не просто академическое знание. Это ключ к написанию быстрых, экономичных и мощных программ.

И в заключение этой вводной мысли — глубокая цитата Станислава Лема: **«...человек, что бы он ни делал, почти никогда не знает, что именно он делает, во всяком случае, не знает до конца»**. На наших занятиях мы постараемся разобраться в том, *как* мы думаем и *как* можно формализовать этот процесс, чтобы научить компьютеры решать задачи за нас.

# О бОльшое

**O-большое** — это специальная нотация, которая описывает, как **скорость работы алгоритма** изменяется в зависимости от размера входных данных. 

Зачем это нужно? На практике вам постоянно придется выбирать между разными алгоритмами для решения одной и той же задачи. O-большое помогает объективно сравнить их эффективность и предсказать, как они будут работать с большими объемами данных.

Главная идея в том, что время выполнения алгоритмов растет с **разной скоростью** при увеличении размера входных данных (`n`).

**Классический пример: Простой поиск vs. Бинарный поиск**

*   **Простой поиск (линейный):** В худшем случае нужно проверить каждый из `n` элементов. Если проверка одного элемента занимает 1 мс, то для 100 элементов потребуется 100 мс, а для 1 000 000 000 элементов — 1 000 000 000 мс (это примерно **11 дней**).
*   **Бинарный поиск (логарифмический):** В худшем случае нужно проверить `log₂n` элементов. Для тех же 1 000 000 000 элементов это всего около 30 проверок (т.к. 2³⁰ ≈ 1 млрд), что займет примерно **30 мс**.

Вывод: При росте `n` бинарный поиск начинает работать **не просто быстрее, а экспоненциально быстрее**. Для миллиарда элементов он оказывается в миллионы раз эффективнее.


O-большое **не измеряет время в секундах**. Оно подсчитывает **количество операций**, которые алгоритму придется выполнить в худшем случае, и показывает, как это количество растет с увеличением `n`.

*   **Простой поиск** требует `n` операций. Его сложность: **O(n)**.
*   **Бинарный поиск** требует `log n` операций. Его сложность: **O(log n)**.

**Задача:** Нарисовать сетку из 16 квадратов.

*   **Алгоритм 1: Рисовать каждый квадрат по отдельности.**
    *   Количество операций (рисование одного квадрата): 16.
    *   Сложность: **O(n)**. Для `n` квадратов нужно `n` шагов.

*   **Алгоритм 2: Складывать лист бумаги.**
    *   Операция: одно сложение. Каждое сложение удваивает количество прямоугольников.
    *   Чтобы получить 16 квадратов (2⁴ = 16), нужно всего 4 сложения.
    *   Сложность: **O(log n)**. Для `n` квадратов нужно `log₂n` шагов.

Этот пример показывает, насколько эффективнее могут быть алгоритмы с O(log n) по сравнению с O(n).

#### **5. O-большое описывает худший случай**

O-большое всегда оценивает производительность в **худшем случае**. Это даёт нам гарантию и понимание "потолка" производительности.

*   **Пример:** Если вы ищете имя в телефонной книге с помощью простого поиска (O(n)), и оно оказывается первым, вы нашли его за 1 операцию. Но в худшем случае (если имя последнее) вам придется просмотреть все `n` имен. Поэтому сложность алгоритма — O(n), а не O(1).


Вот наиболее часто встречающиеся классы сложности:

| Сложность      | Название                      | Как растет количество операций при увеличении `n`               |
| :------------- | :---------------------------- | :-------------------------------------------------------------- |
| **O(1)**       | Константное время             | Не зависит от размера данных                                    |
| **O(log n)**   | Логарифмическое время         | Растет очень медленно                                           |
| **O(n)**       | Линейное время                | Растет пропорционально `n`                                      |
| **O(n log n)** | Линейно-логарифмическое время | Растет чуть быстрее, чем линейно                                |
| **O(n^2)**     | Квадратичное время            | Растет значительно быстрее (для `n=100` нужно ~10 000 операций) |
| **O(n!)**      | Факториальное время           | Растет катастрофически быстро, непригодно для больших `n`       |

#### **7. Ключевые выводы**

1.  **Сравниваем операции, а не секунды:** O-большое сравнивает алгоритмы по количеству операций, а не по абсолютному времени выполнения, которое зависит от мощности компьютера.
2.  **Смотрим на рост:** Нотация показывает, насколько быстро ухудшается производительность алгоритма с ростом размера входных данных.
3.  **Ориентируемся на худший случай:** Это дает нам надежную верхнюю границу времени выполнения.
4.  **O(log n) — друг программиста:** Алгоритмы с такой сложностью невероятно эффективны на больших данных и являются целью для многих оптимизаций.

### O(1)

```python
def is_even(number):
    """Проверка числа на четность"""
    return number % 2 == 0

def get_list_info(arr):
    """Получение основной информации о списке"""
    return {
        'length': len(arr),
        'is_empty': len(arr) == 0,
        'has_elements': len(arr) > 0
    }

# Пример использования
numbers = [1, 2, 3, 4, 5]
print(f"Число 5 четное: {is_even(5)}")  # False
print(f"Информация о списке: {get_list_info(numbers)}")
```

**Пояснение:** Эти операции выполняют фиксированное количество действий независимо от размера входных данных.

### O(log n)

```python
def find_power_of_two(n):
    """Нахождение наибольшей степени двойки, не превышающей n"""
    power = 0
    current = 1
    while current * 2 <= n:
        current *= 2
        power += 1
    return power, current

def integer_square_root(n):
    """Нахождение целого квадратного корня методом бисекции"""
    if n < 2:
        return n
    
    left, right = 1, n
    while left <= right:
        mid = (left + right) // 2
        if mid * mid == n:
            return mid
        elif mid * mid < n:
            left = mid + 1
        else:
            right = mid - 1
    return right

# Пример использования
number = 100
power, value = find_power_of_two(number)
sqrt_val = integer_square_root(number)
print(f"Степень двойки для {number}: 2^{power} = {value}")
print(f"Целый квадратный корень из {number}: {sqrt_val}")
```

#### O(n)

```python
def find_max(arr):
    """Поиск максимального элемента в массиве"""
    if not arr:
        return None
    
    max_val = arr[0]
    for num in arr:
        if num > max_val:
            max_val = num
    return max_val

def count_occurrences(arr, target):
    """Подсчет количества вхождений элемента в массив"""
    count = 0
    for element in arr:
        if element == target:
            count += 1
    return count

# Пример использования
numbers = [3, 7, 2, 7, 1, 7, 4]
max_number = find_max(numbers)
seven_count = count_occurrences(numbers, 7)
print(f"Максимальный элемент: {max_number}")
print(f"Количество семерок: {seven_count}")
```

### O(n log n)

```python
def custom_sort_with_comparison(arr):
    """Сортировка с использованием сложных сравнений"""
    # Эмуляция сложной операции сравнения
    def complex_compare(x, y):
        # Сравниваем по последней цифре, затем по величине
        x_last = x % 10
        y_last = y % 10
        if x_last != y_last:
            return x_last - y_last
        return x - y
    
    # Используем встроенную сортировку с кастомным компаратором
    return sorted(arr, key=lambda x: (x % 10, x))

def find_unique_pairs(arr):
    """Нахождение всех уникальных пар с определенным свойством"""
    # Сначала сортируем для удобства работы с парами
    sorted_arr = sorted(arr)
    pairs = []
    
    # Находим пары, где сумма цифр первого равна сумме цифр второго
    def digit_sum(n):
        return sum(int(d) for d in str(abs(n)))
    
    for i in range(len(sorted_arr)):
        for j in range(i + 1, len(sorted_arr)):
            if digit_sum(sorted_arr[i]) == digit_sum(sorted_arr[j]):
                pairs.append((sorted_arr[i], sorted_arr[j]))
    
    return pairs

# Пример использования
numbers = [123, 45, 67, 89, 234, 56, 78]
sorted_nums = custom_sort_with_comparison(numbers)
unique_pairs = find_unique_pairs(numbers)
print(f"Особенная сортировка: {sorted_nums}")
print(f"Пары с равными суммами цифр: {unique_pairs}")
```

### O(n^2)

```python
def find_all_pairs_sum(arr, target_sum):
    """Поиск всех пар чисел, дающих в сумме заданное значение"""
    pairs = []
    for i in range(len(arr)):
        for j in range(i + 1, len(arr)):
            if arr[i] + arr[j] == target_sum:
                pairs.append((arr[i], arr[j]))
    return pairs

def matrix_transpose(matrix):
    """Транспонирование матрицы (без использования встроенных функций)"""
    if not matrix:
        return []
    
    rows = len(matrix)
    cols = len(matrix[0])
    
    # Создаем новую матрицу для результата
    result = [[0 for _ in range(rows)] for _ in range(cols)]
    
    for i in range(rows):
        for j in range(cols):
            result[j][i] = matrix[i][j]
    
    return result

# Пример использования
numbers = [1, 2, 3, 4, 5]
target = 6
pairs = find_all_pairs_sum(numbers, target)

matrix = [
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
]
transposed = matrix_transpose(matrix)

print(f"Пары с суммой {target}: {pairs}")
print("Транспонированная матрица:")
for row in transposed:
    print(row)
```

### O(n!)

```python
def generate_sequences(n, k):
    """Генерация всех последовательностей длины k из n элементов (с повторениями)"""
    if k == 0:
        return [[]]
    
    sequences = []
    smaller_sequences = generate_sequences(n, k - 1)
    
    for seq in smaller_sequences:
        for i in range(n):
            sequences.append(seq + [i])
    
    return sequences

def count_derangements(n):
    """Подсчет количества беспорядков (derangements) - перестановок без неподвижных точек"""
    if n == 0:
        return 1
    if n == 1:
        return 0
    
    # Рекурсивная формула: !n = (n-1) * (!(n-1) + !(n-2))
    return (n - 1) * (count_derangements(n - 1) + count_derangements(n - 2))

def recursive_permutation_count(n):
    """Рекурсивный подсчет количества перестановок"""
    if n <= 1:
        return 1
    # Каждый вызов порождает n рекурсивных вызовов
    total = 0
    for i in range(n):
        total += recursive_permutation_count(n - 1)
    return total

# Пример использования (используем небольшие n!)
print("Количество последовательностей длины 2 из 3 элементов:")
sequences = generate_sequences(3, 2)
print(f"Всего: {len(sequences)}")
for seq in sequences[:10]:  # Покажем только первые 10
    print(seq)

print(f"\nКоличество беспорядков для n=5: {count_derangements(5)}")
print(f"Рекурсивный подсчет перестановок для n=4: {recursive_permutation_count(4)}")
```

# Сортировка 

До появления компьютеров сортировка данных была известной проблемой, ее ручное выполнение занимало колоссальное количество времени. Когда в 1890-х годах компания Tabulating Machine Company (которая позже стала называться IBM) автоматизировала операции сортировки, это позволило на несколько лет быстрее обработать данные переписи населения США. Существует много алгоритмов сортировки. Более простые имеют временную сложность $O(n^2)$ **Сортировка выбором** — один из таких алгоритмов. Именно его люди предпочитают использовать для сортировки физической колоды карт. Сортировка выбором принадлежит многочисленной группе алгоритмов с квадратичной стоимостью. Мы, как правило, используем их для упорядочивания наборов данных, состоящих меньше чем из 1000 элементов. Одним из известных алгоритмов является сортировка вставками. Он показывает очень хорошую эффективность в сортировке уже почти упорядоченных наборов данных даже очень большого объема.

```java
function insertion_sort(list) 
	for i ← 2 … list.length 
		j ← i 
		while j and list[j-1] > list[j] 
			list.swap_items(j, j-1) 
			j ← j - 1
```

Выполните этот алгоритм на бумаге, с использованием большей частью отсортированного списка чисел. Для массивов, где не упорядочено незначительное число элементов, insertion_sort имеет сложность $O(n)$. В этом случае он выполняет меньше операций, чем какой-либо другой алгоритм сортировки. В отношении крупных наборов данных, о которых нельзя сказать, что они отсортированы большей частью, алгоритмы с временной сложностью $O(n^2)$ оказываются слишком медленными. Здесь нам нужны более эффективные подходы. Самыми известными высокоскоростными алгоритмами сортировки являются сортировка слиянием и так называемая быстрая сортировка, оба имеют сложность $O(n\log n)$. Вот как алгоритм быстрой сортировки раскладывает по порядку колоду карт.

1. Если в колоде менее четырех карт, то упорядочить их — и работа завершена. В противном случае перейти к шагу 2.
2. Вынуть из колоды наугад любую карту, которая становится опорной.
3. Карты со значением больше, чем у опорной, кладутся в новую колоду справа; карты с меньшим значением кладутся в новую колоду слева.
4. Проделать эту процедуру для каждой из двух только что созданных колод.
5. Объединить левую колоду, опорную карту и правую колоду, чтобы получить отсортированную колоду.

![images](card_deck.png)

Перетасуйте колоду карт и проделайте описанные шаги. Это поможет вам опробовать на практике алгоритм быстрой сортировки, а заодно укрепит ваше понимание рекурсии. Теперь вы готовы решать большинство задач, связанных с сортировкой. Здесь мы осветили не все алгоритмы сортировки, так что просто помните, что их гораздо больше и каждый из них соответствует конкретным задачам


# Поиск

Поиск определенной информации в памяти является ключевой операцией в вычислениях. Программисту очень важно владеть алгоритмами поиска. Самый простой из них — последовательный поиск: вы просматриваете все элементы один за другим, пока не будет найден нужный; как вариант, вы можете проверить все элементы, чтобы понять, что искомого среди них нет. Легко заметить, что последовательный поиск имеет сложность $O(n)$, где $n$ — это общее количество элементов в пространстве поиска. Но на случай, когда элементы хорошо структурированы, есть более эффективные алгоритмы. В разделе «Структуры»  мы убедились, что извлечение данных, представленных в формате сбалансированного двоичного дерева поиска, стоит всего $O(log n)$. Если ваши элементы хранятся в сортированном массиве, то их можно отыскать за такое же время, $O(logn)$, посредством двоичного поиска. Этот алгоритм на каждом шаге отбрасывает половину пространства поиска:

```java
function binary_search(items, key)
	if not items
		return NULL
	i ← items.length / 2
	if key = items[i] 
		return items[i] 
	if key > items[i] 
		sliced ← items.slice(i+1, items.length) 
	else 
		sliced ← items.slice(0, i-1) 
	return binary_search(sliced, key)
```

На каждом шаге алгоритм `binary_search` выполняет постоянное число операций и отбрасывает половину входных данных. Это означает, что для n элементов пространство поиска сведется к нулю за $log_2n$ шагов. Поскольку на каждом шаге выполняется постоянное количество операций, алгоритм имеет сложность $O(logn)$. Вы можете выполнять поиск среди миллиона или триллиона элементов, и этот алгоритм по-прежнему будет показывать хорошую производительность.

Впрочем, существуют еще более эффективные алгоритмы. Если элементы хранятся в хеш-таблице, достаточно вычислить хеш-ключ искомого элемента. Этот хеш даст его адрес! Время, необходимое для нахождения элемента, не меняется с увеличением пространства поиска. Не имеет значения, ищете вы среди миллионов, миллиардов или триллионов элементов, — количество операций останется постоянным, а значит, процесс имеет временную сложность $O(1)$, он действует почти мгновенно.
